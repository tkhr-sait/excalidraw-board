# https://docs.litellm.ai/docs/proxy/config_settings
model_list:
  # Responses API models
  - model_name: "claude-3-5-haiku-20241022"
    litellm_params:
      model: openai/qwen3-coder-30b-a3b-instruct
      api_key: "dummy"
      api_base: http://host.docker.internal:1234/v1
      repetition_penalty: 1.05
      temperature: 0.7
      top_k: 20
      top_p: 0.8
    model_info:
      max_tokens: 32768
  - model_name: "qwen3-coder-30b-a3b-instruct"
    litellm_params:
      model: openai/qwen3-coder-30b-a3b-instruct
      api_key: "dummy"
      api_base: http://host.docker.internal:1234/v1
      repetition_penalty: 1.05
      temperature: 0.7
      top_k: 20
      top_p: 0.8
    model_info:
      max_tokens: 262144
  - model_name: "openai/gpt-oss-120b"
    litellm_params:
      model: openai/gpt-oss-120b
      api_key: "dummy"
      api_base: http://host.docker.internal:1234/v1
      repetition_penalty: 1.1
      temperature: 0.8
      top_k: 40
      top_p: 0.8
    model_info:
      max_tokens: 131072
  - model_name: "unsloth/gpt-oss-120b"
    litellm_params:
      model: openai/unsloth/gpt-oss-120b
      api_key: "dummy"
      api_base: http://host.docker.internal:1234/v1
      repetition_penalty: 1.1
      temperature: 0.8
      top_k: 40
      top_p: 0.8
    model_info:
      max_tokens: 131072
